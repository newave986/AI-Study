{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "focused-numbers",
   "metadata": {},
   "source": [
    "# Chapter 04. Multivariate Linear Regression\n",
    "calculate one output prediction value from multiple informations.\n",
    "- Simple Linear Regression Review\n",
    "- Multivariate Linear Regression Theory\n",
    "- Native Data Repressentation\n",
    "- Matrix Data Representation\n",
    "- Multivariate Linear Regression Code\n",
    "- about nn.Module\n",
    "- about F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-duncan",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-johnston",
   "metadata": {},
   "source": [
    "## I. Simple Linear Regression Review\n",
    "one income, one output\n",
    "- but in most case, we need more information -> multivariate could solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-industry",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-shadow",
   "metadata": {},
   "source": [
    "## II. Multivariate Linear Regression\n",
    "more than one income, one output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-mapping",
   "metadata": {},
   "source": [
    "> example data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-salon",
   "metadata": {},
   "source": [
    "|Quiz 1 (x1)|Quiz 2 (x2)|Quiz 3 (x3)|Final (y)|\n",
    "|---|---|---|---|\n",
    "|73|80|75|152|\n",
    "|93|88|93|185|\n",
    "|89|91|80|180|\n",
    "|96|98|100|196|\n",
    "73|66|70|142|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medium-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smooth-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-nowhere",
   "metadata": {},
   "source": [
    "### Hypothesis Function\n",
    "structure of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-gauge",
   "metadata": {},
   "source": [
    "$$ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-christian",
   "metadata": {},
   "source": [
    "if the model has *three inputs*, **weight** would be *three* too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-habitat",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-edition",
   "metadata": {},
   "source": [
    "## III. Native Data Repressentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-mailman",
   "metadata": {},
   "source": [
    "### Hypothesis Function: Naive\n",
    "- the simple hypothesis definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate H(x)\n",
    "hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-topic",
   "metadata": {},
   "source": [
    "- but, how can we code this if x is the vector that length is 1000?\n",
    "\n",
    "we can calculate 3, but when the data is more much, we can't do that with this. (impossible because it would be toooo long)\n",
    "> the answer is matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-portugal",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-object",
   "metadata": {},
   "source": [
    "## IV. Matrix Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-virus",
   "metadata": {},
   "source": [
    "### Hypothesis Function: Matrix\n",
    "- we can calculate Hypothesis Function *at once* by using *matmal()*.\n",
    "    - more simple\n",
    "    - don't need to change code even if the length of x changes.\n",
    "    - faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-tooth",
   "metadata": {},
   "source": [
    "$$ ð»(ð‘¥)=ð‘Šð‘¥ + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate H(x)\n",
    "hypothesis = x_train.matmul(W) + b # or .mm or @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-davis",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-secondary",
   "metadata": {},
   "source": [
    "## V. Multivariate Linear Regression Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-amplifier",
   "metadata": {},
   "source": [
    "### Cost Function: MSE\n",
    "same code with original Simple Linear Regression code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-blair",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-stuart",
   "metadata": {},
   "source": [
    "> $\\sum\\$: mean\n",
    "\n",
    "> $x^{(i)}$: Prediction\n",
    "\n",
    "> $y^{(i)}$: Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-duncan",
   "metadata": {},
   "source": [
    "### Gradient Descent with ```torch.optim```\n",
    "$$ \\nabla W = \\frac{\\partial cost}{\\partial W} = \\frac{2}{m} \\sum^m_{i=1} \\left( Wx^{(i)} - y^{(i)} \\right)x^{(i)} $$\n",
    "$$ W := W - \\alpha \\nabla W $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "# how to use optimizer\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-victory",
   "metadata": {},
   "source": [
    "### Full Code with ```torch.optim(1)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-affiliation",
   "metadata": {},
   "source": [
    "> 1. set data\n",
    "    * only the definition of W is different.\n",
    "> 2. set model\n",
    "> 3. set optimizer\n",
    "> 4. calculate Hypothesis\n",
    "> 5. calculate Cost (with MSE)\n",
    "> 6. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thick-universal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# initialize model\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # calculate H(x)\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    \n",
    "    # calculate cost\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # improve H(x) by cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-moment",
   "metadata": {},
   "source": [
    "result overview\n",
    "- reducing cost -> going closer to 0.\n",
    "- H(x) going closer to y.\n",
    "- It can divergence depending on learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-receptor",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-apparel",
   "metadata": {},
   "source": [
    "## VI. about ```nn.Module```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alien-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# calculate H(x)\n",
    "hypothesis = x_train.matmul(W) + b # or . mm or @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-dispatch",
   "metadata": {},
   "source": [
    "it can be bothered to write all w when the model gets bigger and bigger. </br>\n",
    "```nn.Module``` could provide you a simple, handy module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abandoned-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-cylinder",
   "metadata": {},
   "source": [
    "- import ```torch.nn``` and make model \n",
    "    - it can make you easy to make neural network.\n",
    "- the mean of ```nn.Linear(3, 1)``` is: (it only requires this information)\n",
    "    - input dimension: 3\n",
    "    - output dimension: 1\n",
    "- calculate Hypothesis in ```forward()```\n",
    "- calculation of gradient is that PyTorch automatically does: ```backward()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-crash",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-secretariat",
   "metadata": {},
   "source": [
    "## VII. about F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cost\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-novel",
   "metadata": {},
   "source": [
    "```F.mse_loss``` could provide various cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# calculate cost\n",
    "cost = F.mse_loss(prediction, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-bahrain",
   "metadata": {},
   "source": [
    "- in here, we use *loss function* that ```torch.nn.functional``` provides.\n",
    "- we can easliy modify this with other *loss* \n",
    "    - for example, l1_loss, smooth_l1_loss, or other.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-galaxy",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-kuwait",
   "metadata": {},
   "source": [
    "## Full Code with ```torch.optim```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "continued-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# data\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sized-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "# W = torch.zeros((3, 1), requires_grad = True)\n",
    "# b = torch.zeros(1, requires_grad = True)\n",
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "charming-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    1/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    2/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    3/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    4/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    5/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    6/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    7/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    8/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch    9/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   10/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   11/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   12/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   13/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   14/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   15/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   16/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   17/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   18/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   19/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n",
      "Epoch   20/20 hypothesis: tensor([52.0442, 63.5292, 62.0920, 67.8555, 48.4621]) Cost: 12763.791016\n"
     ]
    }
   ],
   "source": [
    "# set optimizer\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # calculate H(x)\n",
    "    # hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    hypothesis = model(x_train)\n",
    "    \n",
    "    # calculate cost\n",
    "    # cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "    \n",
    "    # improve H(x) by cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-pound",
   "metadata": {},
   "source": [
    "1. set data\n",
    "2. **set model** changed\n",
    "3. set optimizer\n",
    "4. **calculate Hypothesis** changed\n",
    "5. **calculate Cost (with MSE)** changed\n",
    "6. Gradient Descent\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
