{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hourly-franklin",
   "metadata": {},
   "source": [
    "# Chapter 04. Multivariate Linear Regression\n",
    "calculate one output prediction value from multiple informations.\n",
    "- Simple Linear Regression Review\n",
    "- Multivariate Linear Regression Theory\n",
    "- Native Data Repressentation\n",
    "- Matrix Data Representation\n",
    "- Multivariate Linear Regression Code\n",
    "- about nn.Module\n",
    "- about F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-survey",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-berkeley",
   "metadata": {},
   "source": [
    "## I. Simple Linear Regression Review\n",
    "one income, one output\n",
    "- but in most case, we need more information -> multivariate could solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-scottish",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-theta",
   "metadata": {},
   "source": [
    "## II. Multivariate Linear Regression\n",
    "more than one income, one output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-retro",
   "metadata": {},
   "source": [
    "> example data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-reach",
   "metadata": {},
   "source": [
    "|Quiz 1 (x1)|Quiz 2 (x2)|Quiz 3 (x3)|Final (y)|\n",
    "|---|---|---|---|\n",
    "|73|80|75|152|\n",
    "|93|88|93|185|\n",
    "|89|91|80|180|\n",
    "|96|98|100|196|\n",
    "73|66|70|142|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "active-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-guitar",
   "metadata": {},
   "source": [
    "### Hypothesis Function\n",
    "structure of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-transport",
   "metadata": {},
   "source": [
    "$$ H(x) = x_1w_1 + x_2w_2 + x_3w_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-column",
   "metadata": {},
   "source": [
    "if the model has *three inputs*, **weight** would be *three* too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-small",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-context",
   "metadata": {},
   "source": [
    "## III. Native Data Repressentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-reflection",
   "metadata": {},
   "source": [
    "### Hypothesis Function: Naive\n",
    "- the simple hypothesis definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate H(x)\n",
    "hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-finance",
   "metadata": {},
   "source": [
    "- but, how can we code this if x is the vector that length is 1000?\n",
    "\n",
    "we can calculate 3, but when the data is more much, we can't do that with this. (impossible because it would be toooo long)\n",
    "> the answer is matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-liberty",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-relations",
   "metadata": {},
   "source": [
    "## IV. Matrix Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-efficiency",
   "metadata": {},
   "source": [
    "### Hypothesis Function: Matrix\n",
    "- we can calculate Hypothesis Function *at once* by using *matmal()*.\n",
    "    - more simple\n",
    "    - don't need to change code even if the length of x changes.\n",
    "    - faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-mozambique",
   "metadata": {},
   "source": [
    "$$ 𝐻(𝑥)=𝑊𝑥 + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate H(x)\n",
    "hypothesis = x_train.matmul(W) + b # or .mm or @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-induction",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-burke",
   "metadata": {},
   "source": [
    "## V. Multivariate Linear Regression Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-bookmark",
   "metadata": {},
   "source": [
    "### Cost Function: MSE\n",
    "same code with original Simple Linear Regression code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-shelter",
   "metadata": {},
   "source": [
    "$$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-xerox",
   "metadata": {},
   "source": [
    "> $\\sum\\$: mean\n",
    "\n",
    "> $x^{(i)}$: Prediction\n",
    "\n",
    "> $y^{(i)}$: Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-identity",
   "metadata": {},
   "source": [
    "### Gradient Descent with ```torch.optim```\n",
    "$$ \\nabla W = \\frac{\\partial cost}{\\partial W} = \\frac{2}{m} \\sum^m_{i=1} \\left( Wx^{(i)} - y^{(i)} \\right)x^{(i)} $$\n",
    "$$ W := W - \\alpha \\nabla W $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "# how to use optimizer\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-phrase",
   "metadata": {},
   "source": [
    "### Full Code with ```torch.optim(1)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-clone",
   "metadata": {},
   "source": [
    "> 1. set data\n",
    "    * only the definition of W is different.\n",
    "> 2. set model\n",
    "> 3. set optimizer\n",
    "> 4. calculate Hypothesis\n",
    "> 5. calculate Cost (with MSE)\n",
    "> 6. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "knowing-image",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# initialize model\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = torch.optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # calculate H(x)\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    \n",
    "    # calculate cost\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # improve H(x) by cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-violation",
   "metadata": {},
   "source": [
    "result overview\n",
    "- reducing cost -> going closer to 0.\n",
    "- H(x) going closer to y.\n",
    "- It can divergence depending on learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-fundamental",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-diploma",
   "metadata": {},
   "source": [
    "## VI. about nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-vegetation",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-failure",
   "metadata": {},
   "source": [
    "## VII. about F.mse_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
