{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "manual-poetry",
   "metadata": {},
   "source": [
    "# Chapter 05. Logistic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-destination",
   "metadata": {},
   "source": [
    "- Reminder\n",
    "- Computing Hypothesis\n",
    "- Computing Cost Function\n",
    "- Evaluation\n",
    "- Higher Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-climate",
   "metadata": {},
   "source": [
    "## Reminder: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-works",
   "metadata": {},
   "source": [
    "P(x=1) = 1-P(x=0)\n",
    "\n",
    "2-dim size의 vector가 주어졌을 때, 0 또는 1 중에 어떤 쪽에 더 가까운지 구하게 된다.\n",
    "\n",
    "|x · m| = (m, d) x (d, 1) = (m, 1)\n",
    "\n",
    "[0, 1, 0..] - binary classification 문제의 setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-federal",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$\n",
    "\n",
    "### Cost\n",
    "\n",
    "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$\n",
    "\n",
    "If $y \\simeq H(x)$, cost is near 0.\n",
    "\n",
    "If $y \\neq H(x)$, cost is high.\n",
    "\n",
    "\n",
    "$W$을 이용하여 0과 1에 근사하도록 함.\n",
    "\n",
    "H(x) = P(x=1; w) = 1-p(x=0; w)\n",
    "\n",
    "### Weight Updata via Gradient Descent\n",
    "\n",
    "이를 통해 cost function을 훈련시킬 수 있다.\n",
    "\n",
    "$$ W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W) $$\n",
    "$\\alpha$: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "colonial-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "registered-sleeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22a98b94270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-language",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "affecting-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-customer",
   "metadata": {},
   "source": [
    "|x_data| = (6, 2)\n",
    "\n",
    "|y_data| = (6,)\n",
    "\n",
    "m=6, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ceramic-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-bradley",
   "metadata": {},
   "source": [
    "```torch.Tensor``` format으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sharing-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-hanging",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-booking",
   "metadata": {},
   "source": [
    "## Computing the Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-cambodia",
   "metadata": {},
   "source": [
    "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$\n",
    "\n",
    "=> $ H(X) = \\frac{1}{1+e^{-X·W}} $\n",
    "\n",
    "=> $ H(X) = \\frac{1}{1+e^{-X·W + b}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-bathroom",
   "metadata": {},
   "source": [
    "### torch.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-charity",
   "metadata": {},
   "source": [
    "PyTorch has a ```torch.exp()``` function that resembles the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "genuine-wyoming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals:  tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-ending",
   "metadata": {},
   "source": [
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "similar-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "billion-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-basin",
   "metadata": {},
   "source": [
    "```x_train.matmul(W)``` - $X·W$ = ```torch.matmal(X, W)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "polish-hygiene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-plane",
   "metadata": {},
   "source": [
    "### sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "sporting-johnson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/(1+e^{-1}) equals:  tensor([0.7311])\n"
     ]
    }
   ],
   "source": [
    "print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "assured-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "beneficial-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-baker",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-evanescence",
   "metadata": {},
   "source": [
    "## Computing the Cost Function  (Low-Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-dublin",
   "metadata": {},
   "source": [
    "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-poison",
   "metadata": {},
   "source": [
    "```hypothesis```와 ```y_train```의 차이를 알고자 한다(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "everyday-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-impossible",
   "metadata": {},
   "source": [
    "### Loss of One Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "monetary-gather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y_train[0] * torch.log(hypothesis[0]) + \n",
    "  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-algorithm",
   "metadata": {},
   "source": [
    "hypothesis[0]: scalar = logP(x=1; w)\n",
    "\n",
    "1-hypothesis[0]: logP(x=0; w) = 1-logP(x=1; w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-format",
   "metadata": {},
   "source": [
    "### Loss of Entire Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "nonprofit-bennett",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train * torch.log(hypothesis) + \n",
    "           (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "registered-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-portal",
   "metadata": {},
   "source": [
    "### ```F.binary_cross_entropy``` pytorch simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "municipal-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-transaction",
   "metadata": {},
   "source": [
    "위에서 시행했던 코드를 ```F.binary_cross_entropy``` 함수를 이용하여 한 번에 표현 가능함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-listening",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-baseline",
   "metadata": {},
   "source": [
    "## Training Procedure Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-princess",
   "metadata": {},
   "source": [
    "### low level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "immediate-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "piano-nurse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031673\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1) # Optimization 수행\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = -(y_train * torch.log(hypothesis) + \n",
    "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # Zero Gradient 해 주기\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-midnight",
   "metadata": {},
   "source": [
    "### ```F.binary_cross_entropy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "alpha-chick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031672\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-knitting",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-morning",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-director",
   "metadata": {},
   "source": [
    "모델 학습이 끝난 후, training set과 우리의 model이 얼마나 잘 맞는지 확인하기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-wichita",
   "metadata": {},
   "source": [
    "### hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "flying-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-onion",
   "metadata": {},
   "source": [
    "hypothesis[:5]로 다섯 개만 출력하였으니 값은 다섯 개만 출력됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-amendment",
   "metadata": {},
   "source": [
    "```hypothesis```을 ```binary prediection```으로 바꿈. (0.5와 비교한 것을 기준으로)\n",
    "    \n",
    "- range of hypothesis: [0, 1] R\n",
    "\n",
    "- range of binary predictions: 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-burns",
   "metadata": {},
   "source": [
    "### binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "physical-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-appearance",
   "metadata": {},
   "source": [
    "### compare with y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "color-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(prediction[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "accepting-samoa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-naples",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "mexican-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an accuracy of 100.00% for the training set.\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-slovakia",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-bumper",
   "metadata": {},
   "source": [
    "## Higher Implementation with Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-charleston",
   "metadata": {},
   "source": [
    "```nn.Module``` 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-colombia",
   "metadata": {},
   "source": [
    "### logistic regression 행하는 class 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "crazy-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "addressed-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "noble-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.548766 Accuracy 83.33%\n",
      "Epoch   10/100 Cost: 0.821105 Accuracy 66.67%\n",
      "Epoch   20/100 Cost: 0.577588 Accuracy 83.33%\n",
      "Epoch   30/100 Cost: 0.483939 Accuracy 83.33%\n",
      "Epoch   40/100 Cost: 0.398343 Accuracy 83.33%\n",
      "Epoch   50/100 Cost: 0.317014 Accuracy 83.33%\n",
      "Epoch   60/100 Cost: 0.243594 Accuracy 83.33%\n",
      "Epoch   70/100 Cost: 0.187757 Accuracy 100.00%\n",
      "Epoch   80/100 Cost: 0.157877 Accuracy 100.00%\n",
      "Epoch   90/100 Cost: 0.144273 Accuracy 100.00%\n",
      "Epoch  100/100 Cost: 0.134425 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
